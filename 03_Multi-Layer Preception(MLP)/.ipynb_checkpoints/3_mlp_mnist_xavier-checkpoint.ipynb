{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deeper multilayer perception with xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-ef0c57ba93ed>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/liuzhiyang/.conda/envs/jupyter/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/liuzhiyang/.conda/envs/jupyter/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/liuzhiyang/.conda/envs/jupyter/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/liuzhiyang/.conda/envs/jupyter/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/liuzhiyang/.conda/envs/jupyter/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "PACKAGES LOADED\n"
     ]
    }
   ],
   "source": [
    "# load package and mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data/', one_hot=True)\n",
    "%matplotlib inline  \n",
    "print (\"PACKAGES LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xavier init\n",
    "\"\"\"Set the parameter initialization using the method described.\n",
    "This method is designed to keep the scale of the gradients roughly the same\n",
    "in all layers.\n",
    "Xavier Glorot and Yoshua Bengio (2010):\n",
    "       Understanding the difficulty of training deep feedforward neural\n",
    "       networks. International conference on artificial intelligence and\n",
    "       statistics.\n",
    "Args:\n",
    "n_inputs: The number of input nodes into each output.\n",
    "n_outputs: The number of output nodes for each input.\n",
    "uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "Returns:\n",
    "An initializer.\n",
    "\"\"\"\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    if uniform:\n",
    "        # 6 is used in the paper\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_inputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        # 3 gives us approximately the same limits as above since this repicks\n",
    "        # values greater than 2 standard deviations from the mean.\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liuzhiyang/.conda/envs/jupyter/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# parameters \n",
    "learning_rate = 0.001\n",
    "training_epoch = 50\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "\n",
    "# network parameters\n",
    "n_input = 784  # 28 x 28\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 256 # 2nd layer num features\n",
    "n_hidden_3 = 256 # 3rd layer num features\n",
    "n_hidden_4 = 256 # 4th layer num features\n",
    "n_classes = 10  # classes(0 ~9 digits)\n",
    "\n",
    "# tf graph\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "dropout_keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# weight and bias\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.get_variable(\"h1\", shape=[n_input, n_hidden_1],    initializer=xavier_init(n_input,n_hidden_1)),\n",
    "    'h2': tf.get_variable(\"h2\", shape=[n_hidden_1, n_hidden_2], initializer=xavier_init(n_hidden_1,n_hidden_2)),\n",
    "    'h3': tf.get_variable(\"h3\", shape=[n_hidden_2, n_hidden_3], initializer=xavier_init(n_hidden_2,n_hidden_3)),\n",
    "    'h4': tf.get_variable(\"h4\", shape=[n_hidden_3, n_hidden_4], initializer=xavier_init(n_hidden_3,n_hidden_4)),\n",
    "    'out': tf.get_variable(\"out\", shape=[n_hidden_4, n_classes], initializer=xavier_init(n_hidden_4,n_classes))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-57b33b46818a>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-4-57b33b46818a>:12: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "network ready\n"
     ]
    }
   ],
   "source": [
    "def multilayer_perception(_x, _weights, _biases, _keep_prob):\n",
    "    layer_1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(_x, _weights['h1']), _biases['b1'])), _keep_prob)\n",
    "    layer_2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])), _keep_prob)\n",
    "    layer_3 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_2, _weights['h3']), _biases['b3'])), _keep_prob)\n",
    "    layer_4 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer_3, _weights['h4']), _biases['b4'])), _keep_prob)\n",
    "    return(tf.add(tf.matmul(layer_4, _weights['out']), _biases['out']))\n",
    "\n",
    "# construct model\n",
    "pred = multilayer_perception(x, weights, biases, dropout_keep_prob)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"network ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [005/050]  avg loss: 0.06288  train accuracy: 1.000  test accuracy: 0.975  \n",
      "Epoch: [010/050]  avg loss: 0.03345  train accuracy: 1.000  test accuracy: 0.979  \n",
      "Epoch: [015/050]  avg loss: 0.01984  train accuracy: 0.990  test accuracy: 0.981  \n",
      "Epoch: [020/050]  avg loss: 0.01446  train accuracy: 1.000  test accuracy: 0.981  \n",
      "Epoch: [025/050]  avg loss: 0.01002  train accuracy: 0.990  test accuracy: 0.981  \n",
      "Epoch: [030/050]  avg loss: 0.00790  train accuracy: 1.000  test accuracy: 0.983  \n",
      "Epoch: [035/050]  avg loss: 0.00621  train accuracy: 1.000  test accuracy: 0.983  \n",
      "Epoch: [040/050]  avg loss: 0.00539  train accuracy: 0.990  test accuracy: 0.982  \n",
      "Epoch: [045/050]  avg loss: 0.00463  train accuracy: 1.000  test accuracy: 0.983  \n",
      "Epoch: [050/050]  avg loss: 0.00447  train accuracy: 1.000  test accuracy: 0.982  \n"
     ]
    }
   ],
   "source": [
    "# launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# training \n",
    "for epoch in range(training_epoch):\n",
    "    avg_loss = 0\n",
    "    n_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for batch in range(n_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        train_feed = {x: batch_xs, y: batch_ys, dropout_keep_prob: 0.7}\n",
    "        sess.run(optimizer, feed_dict=train_feed)\n",
    "        feed = {x: batch_xs, y: batch_ys, dropout_keep_prob: 1.0}\n",
    "        avg_loss += sess.run(loss, feed_dict=feed)\n",
    "    avg_loss = avg_loss / n_batch\n",
    "    \n",
    "    # display\n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        train_accur = sess.run(accuracy, feed_dict=feed)\n",
    "        test_feed = {x: mnist.test.images, y: mnist.test.labels, dropout_keep_prob: 1.0}\n",
    "        test_accur = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"Epoch: [%03d/%03d]  \" \"avg loss: %.5f  \" \"train accuracy: %.3f  \" \"test accuracy: %.3f  \"\n",
    "             %(epoch + 1, training_epoch, avg_loss, train_accur, test_accur))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始learning_rate设置为0.01，loss会一直上升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter]",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
